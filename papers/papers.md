| Year | Title | Venue |  Abstract | Contribution |
|------|-------|-------|-----------|-------------|
| 2025 | **HOT3D: A Multi-View Egocentric Dataset for 3D Hand and Object Tracking** | CVPR | 介绍了HOT3D，一个公开的自我中心3D手部与物体跟踪数据集。数据集包含超过833分钟（370万+ 图像），涵盖19名参与者与33种刚体物体的交互动作，包括简单拾取、观察、放下以及厨房、办公室、客厅环境中的常规动作。数据包含多视角 RGB/单色图像、眼动信号、场景点云以及相机、手部和物体的三维姿态。手部标注采用UmeTrack和MANO格式，物体以PBR材质的3D网格表示。实验显示，多视角方法在 3D手部跟踪、6DoF物体位姿估计以及未知在手物体3D提升任务中显著优于单视角方法。| 数据集发布 |
| 2025 | **Ego4o: Multi-modal Egocentric Human Motion Capture and Understanding** | CVPR | 提出Ego4o框架，通过结合来自可穿戴设备的多模态输入（IMU、第一人称图像、文本描述），实现人体运动捕捉与理解。方法在部分输入和多模态融合下均保持高精度，并可利用 LLM 生成文本描述辅助运动捕捉。实验结果表明，在运动跟踪精度和描述质量上显著优于现有方法。 | 方法创新 |
| 2025 | **EgoPressure: A Multi-Modal Egocentric Dataset for Touch Contact and Pressure Hand-Object Interactions** | CVPR | 提出EgoPressure，一个自我中心手-物交互数据集，专注于触觉接触与压力信息。数据集提供每个接触点的高分辨率压力强度标注，以及通过多视角序列优化方法获得的高精度手部mesh。数据由21名参与者录制，使用一台头戴相机和七台固定Kinect摄像头同步采集RGB图像和深度图，共5小时交互记录。提供基线模型用于从RGB图像估计施加在外部表面的压力，支持联合手部mesh与压力估计。实验表明压力与手部姿态信息互为补充，有助于理解手-物交互。 | 数据集发布 |
| 2025 | **ANNEXE: Unified Analyzing, Answering, and Pixel Grounding for Egocentric Interaction** | CVPR | 提出Ego-IRG任务（Egocentric Interaction Reasoning and pixel Grounding），输入为第一人称图像与查询，实现分析、回答和像素级 grounding 三步操作，生成文本与像素级响应。构建Ego-IRGBench数据集，包含20k+图像和1.6M查询及多模态标注。提出ANNEXE模型，利用多模态大语言模型生成文本和像素输出，实现全面的自我中心交互理解。实验显示 ANNEXE 在 Ego-IRGBench 上优于现有方法。 | 数据集发布 + 方法创新  |
| 2025 | **HUMOTO: A 4D Dataset of Mocap Human Object Interactions** | arXiv | 提出 HUMOTO 数据集，包含 736 个序列（7,875 秒，30 fps），覆盖 63 个精确建模物体和 72 个关节部件，支持动作生成、计算机视觉与机器人研究。数据通过场景驱动 LLM 脚本生成完整任务，并使用 mocap 与相机捕捉解决遮挡问题。数据经过专业艺术家清理，保留物理精度与任务逻辑流。提供多任务基准以推进真实人-物交互建模。 | 数据集发布 |
| 2025 | **HOIGen-1M: A Large-scale Dataset for Human-Object Interaction Video Generation** | CVPR | 提出 HOIGen-1M 数据集，首个大规模 HOI 视频生成数据集，包含 100 万条高质量视频，并配有精确文本描述。采用多模态大语言模型 (MLLMs) 自动筛选视频并人工清理，同时设计 MoME 方法生成精确文本描述，消除模型 hallucination。提出两种评估指标用于粗到细的 HOI 视频生成质量评测。 | 数据集发布 |
| 2025 | **EgoMe: A New Dataset and Challenge for Following Me via Egocentric View in Real World** | arXiv | 提出 EgoMe 数据集，支持研究机器人模仿学习，包含 7,902 对 exo-ego 视频（共 15,804 视频），覆盖多样化日常行为。每对视频包含观察者（exo）和模仿者（ego）视角，同时包含眼动、IMU 多模态传感器及多层标注。提供一套基准任务以促进模仿学习研究，分析显示在多模态关联与行为学习上优于现有数据集。 | 数据集发布 |
| 2025 | **HD-EPIC: A Highly-Detailed Egocentric Video Dataset** | CVPR | 提出 HD-EPIC 数据集，包含 41 小时厨房场景的第一视角视频，覆盖 9 个厨房、69 个菜谱、59K 细粒度动作、51K 音频事件、20K 物体移动以及 37K 物体 mask，所有标注通过数字孪生实现 3D 对齐，并辅以注视信息。数据集用于评测配方理解、动作识别、声音识别、3D 感知与视频对象分割等任务，提供 26K VQA 问题作为挑战基准。 | 数据集发布 |
| 2024 | **Instance Tracking in 3D Scenes from Egocentric Videos** | CVPR | 提出IT3DEgo，一个新的基准数据集和评测协议，用于自我中心场景下的三维实例跟踪。数据集包含RGB与深度视频、逐帧相机位姿，以及二维与三维坐标中的实例级标注。论文提出两种实例注册模式（单视角在线注册、多视角预注册），并比较了基于单目标跟踪（SOT）的二维方法和基于预训练模型的候选生成+匹配方法。实验结果表明，后者在自我中心场景下显著优于SOT方法。 | 数据集发布 + 评测基准 + 基线方法 |
| 2024 | **Learning to Segment Referred Objects from Narrated Egocentric Videos** | CVPR | 提出 Narration-based Video Object Segmentation (NVOS) 任务，旨在在弱监督下利用第一人称视频及其文字描述实现物体实例分割。与以往仅预测边界框的方法不同，ROSA实现了像素级语义对齐。方法结合视频-文本对比损失与区域-短语对比损失，利用大规模视觉-语言模型对区域mask与文本短语进行嵌入对齐。为支持评测，构建了 **VISOR-NVOS** 数据集，包含VISOR的分割标注与新增1.46万条物体级视频解说文本。实验结果显示，ROSA在零样本像素级grounding上显著优于现有方法，并可泛化到第三人称数据集YouCook2。 | 方法创新 + 评测基准 + 数据集发布 |
| 2024 | **Benchmarks and Challenges in Pose Estimation for Egocentric Hand Interactions with Objects** | ECCV | 针对第一视角手-物交互的 3D 重建研究, 设计 HANDS23 挑战赛，基于 AssemblyHands 和 ARCTIC 数据集，分析手-物交互重建中的难点：遮挡、相机畸变、头动模糊；提出 transformer 与多视角融合策略 | 新任务 + 方法创新 |
| 2024 | **EMAG: Ego-motion Aware and Generalizable 2D Hand Forecasting from Egocentric Videos** | ECCV | 针对第一视角视频中手部未来位置预测，提出 EMAG 方法：通过单应矩阵建模自运动，结合光流、手和物体轨迹等多模态特征，提升跨场景泛化能力；在 Ego4D 和 EPIC-Kitchens 55 上取得明显提升 | 多模态融合 + 方法创新 |
| 2024 | **Diffusion-Based Synthetic Dataset Generation for Egocentric 3D Human Pose Estimation** | ECCV | 针对第一视角 3D 人体姿态估计训练数据难以获取、真实数据稀缺的问题，提出基于 Diffusion + ControlNet 的合成数据生成方法，结合少量带文本描述的真实数据，缩小合成与真实数据域间差距；在 SceneEgo 和 GlobalEgoMocap 上显著提升 Mo2Cap2 的精度 | 数据集生成 |
| 2024 | **Are Synthetic Data Useful for Egocentric Hand-Object Interaction Detection?** | ECCV | 探讨合成数据在第一视角手-物交互检测中的作用。通过在 VISOR、EgoHOS 和 ENIGMA-51 数据集上的实验，验证仅使用 10% 真实标注数据时，合成数据能显著提升性能。 | 数据集合成 + 评测基准 |
| 2024 | **3D Hand Pose Estimation in Everyday Egocentric Images** | ECCV | 在日常第一视角图像中进行三维手部姿态估计面临视觉信号差、透视畸变和缺乏三维标注等挑战。本文探索了手部裁剪输入、相机信息、辅助监督和数据集扩展的影响，并提出 WildHands 系统。零样本评估表明在 H2O、AssemblyHands、Epic-Kitchens、Ego-Exo4D 数据集上，方法在二维和三维指标上性能提升 7.4%-66%，在 ARCTIC 数据集上取得最佳表现，同时模型尺寸缩小 10 倍，训练数据量减少 5 倍。 | 方法创新 |
| 2024 | **Ego-Exo4D: Understanding Skilled Human Activity from First- and Third-Person Perspectives** | arXiv | 提出 Ego-Exo4D，一个大规模多模态多视角数据集，包含 740 名参与者在 123 个自然场景下执行熟练人类活动的视频，涵盖 Egocentric 与 Exocentric 视频、多通道音频、眼动注视、3D 点云、IMU、相机位姿及多种语言描述。提供精细化动作理解、熟练度估计、视角转换和 3D 手/身体姿态等基准任务 | 数据集发布 + 基准任务设计 |
| 2024 | **EgoBody3M: Egocentric Body Tracking on a VR Headset using a Diverse Dataset** | ECCV | 提出 EgoBody3M，一种用于 VR 头显的无控制器第一视角人体追踪方法。通过多视角隐特征建模用户身体运动历史，实现高精度实时人体追踪。提供首个大规模真实图像数据集，涵盖多样化被试和动作 | 数据集发布 + 方法创新 |
| 2023 | **Hierarchical Temporal Transformer for 3D Hand Pose Estimation and Action Recognition from Egocentric RGB Videos** | CVPR | 针对自我中心RGB视频中的动态手部动作和行为识别问题，由于自遮挡和模糊性，提出一个基于Transformer的分层时序框架。该框架通过两级级联Transformer编码器实现短期手部姿态估计和长期动作识别，充分利用不同时间尺度的语义关联。方法在FPHA和H2O数据集上取得竞争性结果，并通过大量消融实验验证设计合理性。| 方法创新 |
| 2023 | **MMG-Ego4D: Multi-Modal Generalization in Egocentric Action Recognition** | CVPR | 本文研究了自我中心动作识别中的多模态泛化问题（MMG），旨在探索当某些模态的数据缺失或有限时，系统如何有效泛化。提出两个场景：缺失模态泛化和跨模态零样本泛化。构建 MMG-Ego4D 数据集（源自 Ego4D，包含视频、音频、IMU数据，并经过重新标注），并评估多种模型，提出融合模块、对比对齐训练和跨模态原型损失以提高少样本性能。 | 数据集发布 + 方法创新 + 评测基准 |
| 2023 | **Tracking Multiple Deformable Objects in Egocentric Videos** | TBD | 提出 **DogThruGlasses**，一个大规模可变形多目标跟踪数据集，包含150个视频和73K帧标注，均由智能眼镜采集。同时提出 **DETracker** 方法，可联合检测和跟踪自我中心视频中的可变形物体。方法设计了三个模块：运动解耦网络（MDN）、补丁关联网络（PAN）和补丁记忆网络（PMN），显式处理严重自我运动并跟踪快速变形目标。DETracker 可端到端训练，达到近实时速度，并在 DogThruGlasses 与 YouTube-Hand 数据集上优于现有方法。 | 数据集发布 + 方法创新 |
| 2023 | **AssemblyHands: Towards Egocentric Activity Understanding via 3D Hand Pose Estimation** | CVPR | 提出 AssemblyHands，一个大规模自我中心活动基准数据集，包含高精度3D手部姿态标注，用于研究手-物体交互。数据集包含与 Assembly101 同步采集的自我中心和第三人称图像。设计标注流程：先人工标注训练模型，再自动标注大规模数据，采用多视角特征融合和迭代优化，平均关键点误差 4.20mm，比原 Assembly101 标注误差低 85%。数据集提供 3.0M 图像（49万自我中心图像），建立单视图3D手部姿态估计基线，并设计动作分类任务评估预测手势。实验显示，高质量手部姿态直接提升动作识别能力。 | 数据集发布 + 方法创新 + HOI任务评测 |
| 2023 | **Scene-aware Egocentric 3D Human Pose Estimation** | CVPR | 提出场景感知自我中心3D人体姿态估计方法，利用场景约束引导姿态预测。方法包括：1）自我中心深度估计网络，从广角鱼眼相机预测场景深度并通过深度修复减轻人体遮挡；2）场景感知姿态估计网络，将2D图像特征与深度映射投影到体素空间，用V2V网络回归3D姿态，实现人体与场景几何约束。为训练网络，构建了合成数据集 EgoGTA 和真实场景数据集 EgoPW-Scene。实验显示，预测的3D姿态在人体-场景交互上准确且物理可行，定量和定性上均优于现有方法。 | 方法创新 + 数据集发布 + HOI任务评测 |
| 2022 | **HOI4D: A Large-Scale 4D Egocentric Dataset for Category-Level Human-Object Interaction** | CVPR | 提出 HOI4D，一个大规模4D自我中心数据集，旨在推动类别级人体-物体交互研究。数据集包含2.4M RGB-D视频帧，4000序列，4名参与者与800个不同物体实例（16类）在610个室内房间中的交互。提供帧级全景分割、动作分割、3D手部姿态、类别级物体位姿及手部动作标注，并附物体网格与场景点云。基于HOI4D，建立了三类基准任务：4D 动态点云序列语义分割、类别级物体位姿跟踪和自我中心动作分割。分析显示HOI4D对现有方法提出挑战并提供丰富研究机会。 | 数据集发布 |
| 2022 | **Egocentric Prediction of Action Target in 3D** | CVPR | 本文关注从自我中心视觉中尽早预测用户物体操作动作的3D目标位置，对人机协作等场景尤为重要。为推动该任务研究，构建了一个大规模多模态数据集，包含超过100万帧的RGB-D和IMU流，并提供基于高质量半自动标注的2D和3D标签的评测指标。同时设计了基线方法（RNN）并进行消融实验验证其有效性。实验结果表明，该任务具有重要研究价值。 | 方法创新 + 数据集发布 |
| 2022 | **Ego4D: Around the World in 3,000 Hours of Egocentric Video** | CVPR | 本文介绍 Ego4D，一个大规模自我中心视频数据集及评测套件。数据集包含3,670小时日常生活视频，覆盖数百种场景（家庭、户外、工作、休闲等），由931名来自全球74个地点、9个国家的参与者采集。部分视频附有音频、环境3D网格、眼动、立体视觉或多摄像头同步视频。数据收集严格遵循隐私和伦理标准。基于该数据集，构建了针对过去（记忆查询）、现在（手-物体操作分析、音视频对话、社交交互）和未来（动作预测）的多种基准任务，推动第一人称视觉理解研究。 | 数据集发布 + 多任务基准定义 |
| 2022 | **Joint Hand Motion and Interaction Hotspots Prediction from Egocentric Videos** | CVPR | 提出在自我中心视频中预测未来手-物交互的方法。不同于传统预测动作类别或像素，本方法直接预测手部运动轨迹以及下一个活跃物体的交互热点（interaction hotspots），为未来交互提供低维、具体的描述。为了实现该任务，首先设计自动标注大规模轨迹与热点的方法，然后基于这些数据训练 Object-Centric Transformer (OCT) 模型，通过 Transformer 的自注意力机制进行手-物交互推理，并提供概率框架以采样未来轨迹和热点，从而处理预测的不确定性。实验在 Epic-Kitchens-55、Epic-Kitchens-100 和 EGTEA Gaze+ 数据集上进行，结果显示 OCT 在性能上显著优于现有方法。 | 方法创新 + 新任务定义 + 多数据集评测 |
| 2022 | **E2(GO)MOTION: Motion Augmented Event Stream for Egocentric Action Recognition** | CVPR | 提出在自我中心动作识别中使用事件摄像机（event camera）的方法。事件摄像机以异步方式捕获像素强度变化，几乎无运动模糊，具有超高时间分辨率且功耗低，非常适合穿戴设备上的快速动作识别任务。文中提出 N-EPIC-Kitchens 数据集，是 EPIC-Kitchens 的事件摄像机扩展版本。提出两种方法策略：(i) 直接用传统视频处理网络处理事件数据（E2(GO)），(ii) 利用事件数据蒸馏光流信息（E2(GO)MO）。实验表明，事件数据可在推理阶段无需额外光流计算的情况下，提供与 RGB+光流 相当的性能，并相较于单 RGB 提升最高 4%。 | 数据集扩展 + 方法创新 + 多模态融合 |
| 2022 | **Estimating Egocentric 3D Human Pose in the Wild with External Weak Supervision** | CVPR | 本文提出 EgoPW（Egocentric Poses in the Wild）数据集，用于第一人称 3D 人体姿态估计。数据集由头戴鱼眼相机和辅助外部相机拍摄，解决了 in-the-wild 数据稀缺问题。方法通过外部视角弱监督生成伪标签，并用此训练自我中心姿态估计网络，同时引入新的学习策略以利用高质量外部特征监督第一人称特征。实验表明，该方法在单帧 in-the-wild 图像上准确预测 3D 姿态，定量和定性均优于现有方法。 | 数据集发布 + 方法创新 + 弱监督策略 |
| 2022| **Generative Adversarial Network for Future Hand Segmentation from Egocentric Video** | ECCV | 提出预测第一视角视频中未来手部掩码的任务。方法 EgoGAN 结合 3D 全卷积网络学习时空特征表示，并用 GAN 生成未来头部运动，从而预测未来手部掩码。在 EPIC-Kitchens 和 EGTEA Gaze+ 数据集上验证了效果 | 方法创新 |
| 2022 | **Fine-Grained Egocentric Hand-Object Segmentation: Dataset, Model, and Applications** | ECCV | 提出针对第一视角视频中手与物体的细粒度分割任务，提供 11,243 张带手-物体接触边界的标注图像。提出上下文感知的数据增强方法以适应不同分布的 YouTube 视频，并展示了模型在手状态分类、视频动作识别、3D 手-物体重建、视频修复等下游任务中的应用 | 数据集发布 + 方法创新 |
| 2021 | **Multi-Modal Temporal Convolutional Network for Anticipating Actions in Egocentric Videos** | CVPR | 本文提出一种多模态时间卷积网络（MM-TCN）用于第一人称视频中的动作预测。方法采用层级时间卷积而非循环网络，以保证预测速度，同时通过多模态融合捕捉 RGB、光流和物体特征间的交互。实验在 EPIC-Kitchens-55 和 EPIC-Kitchens-100 数据集上表明，该方法在准确性上与现有方法相当，同时显著提升了推理速度。 | 方法创新 + 多模态融合 + 高效动作预测 |
| 2021 | **H2O: Two Hands Manipulating Objects for First Person Interaction Recognition** | ICCV | 提出针对第一视角下双手操作物体的交互识别任务，提供 H2O 数据集，包括两只手的 3D 姿态、物体的 6D 姿态、多视角 RGB-D 图像、交互标签、物体类别、相机位姿、物体网格和场景点云。提出基于图卷积网络的联合手-物体姿态估计方法，实现一流的第一视角交互识别 | 数据集发布 |
| 2020 | **Generalizing Hand Segmentation in Egocentric Videos with Uncertainty-Guided Model Adaptation** | CVPR | 本文针对第一人称视频中手部分割的泛化问题提出一种贝叶斯 CNN 模型自适应框架，无需目标域的分割标签。方法引入两个关键因素：1）模型在新域上的预测不确定性，2）跨域共享的手部形状信息。基于此，提出迭代自训练方法，并结合对抗模块约束模型适应过程。实验证明该方法在多个自我中心数据集上显著提升手部分割的泛化性能。 | 方法创新 |